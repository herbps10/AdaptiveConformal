@misc{feldman2023rollingrisk,
  title={Achieving Risk Control in Online Learning Settings},
  author={Shai Feldman and Liran Ringel and Stephen Bates and Yaniv Romano},
  year={2023},
  eprint={arXiv preprint arXiv:2205.09095},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2205.09095}
}
@InProceedings{zaffran2022agaci,
  title={Adaptive Conformal Predictions for Time Series},
  author={Zaffran, Margaux and Feron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
  booktitle={Proceedings of the 39th International Conference on Machine Learning},
  pages={25834--25866},
  year={2022},
  editor={Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume={162},
  series={Proceedings of Machine Learning Research},
  month={17--23 Jul},
  publisher={PMLR},
  pdf= 	 {https://proceedings.mlr.press/v162/zaffran22a/zaffran22a.pdf},
  url= 	 {https://proceedings.mlr.press/v162/zaffran22a.html}
}
@misc{gibbs2022faci,
  title={Conformal Inference for Online Prediction with Arbitrary Distribution Shifts},
  author={Isaac Gibbs and Emmanuel Candès},
  year={2022},
  eprint={arXiv preprint arXiv:2208.08401},
  primaryClass={stat.ME},
  url={https://arxiv.org/abs/2208.08401}
}
@misc{bhatnagar2023improved,
      title={Improved Online Conformal Prediction via Strongly Adaptive Online Learning},
      author={Aadyot Bhatnagar and Huan Wang and Caiming Xiong and Yu Bai},
      year={2023},
      eprint={arXiv preprint arXiv:2302.07869},
      primaryClass={cs.LG}
}
@article{orabona2018scalefree,
  title = {Scale-free online learning},
  journal = {Theoretical Computer Science},
  volume = {716},
  pages = {50-69},
  year = {2018},
  note = {Special Issue on ALT 2015},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/j.tcs.2017.11.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397517308514},
  author = {Francesco Orabona and Dávid Pál},
  keywords = {Online algorithms, Optimization, Regret bounds, Online learning},
  abstract = {We design and analyze algorithms for online linear optimization that have optimal regret and at the same time do not need to know any upper or lower bounds on the norm of the loss vectors. Our algorithms are instances of the Follow the Regularized Leader (FTRL) and Mirror Descent (MD) meta-algorithms. We achieve adaptiveness to the norms of the loss vectors by scale invariance, i.e., our algorithms make exactly the same decisions if the sequence of loss vectors is multiplied by any positive constant. The algorithm based on FTRL works for any decision set, bounded or unbounded. For unbounded decisions sets, this is the first adaptive algorithm for online linear optimization with a non-vacuous regret bound. In contrast, we show lower bounds on scale-free algorithms based on MD on unbounded domains.}
}
