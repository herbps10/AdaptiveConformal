@article{feldman2023rollingrisk,
title={Achieving Risk Control in Online Learning Settings},
author={Shai Feldman and Liran Ringel and Stephen Bates and Yaniv Romano},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=5Y04GWvoJu},
note={}
}


@InProceedings{zaffran2022agaci,
  title={Adaptive Conformal Predictions for Time Series},
  author={Zaffran, Margaux and Feron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
  booktitle={Proceedings of the 39th International Conference on Machine Learning},
  pages={25834--25866},
  year={2022},
  editor={Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume={162},
  series={Proceedings of Machine Learning Research},
  month={17--23 Jul},
  publisher={PMLR},
  pdf= 	 {https://proceedings.mlr.press/v162/zaffran22a/zaffran22a.pdf},
  url= 	 {https://proceedings.mlr.press/v162/zaffran22a.html}
}

@inproceedings{gibbs2021aci,
 author = {Gibbs, Isaac and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1660--1672},
 publisher = {Curran Associates, Inc.},
 title = {Adaptive Conformal Inference Under Distribution Shift},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{gibbs2022faci,
  title={Conformal Inference for Online Prediction with Arbitrary Distribution Shifts},
  author={Isaac Gibbs and Emmanuel Candès},
  year={2022},
  eprint={arXiv preprint arXiv:2208.08401},
  primaryClass={stat.ME},
  url={https://arxiv.org/abs/2208.08401}
}

@inproceedings{bhatnagar2023improved,
author = {Bhatnagar, Aadyot and Wang, Huan and Xiong, Caiming and Bai, Yu},
title = {Improved Online Conformal Prediction via Strongly Adaptive Online Learning},
year = {2023},
publisher = {JMLR.org},
abstract = {We study the problem of uncertainty quantification via prediction sets, in an online setting where the data distribution may vary arbitrarily over time. Recent work develops online conformal prediction techniques that leverage regret minimization algorithms from the online learning literature to learn prediction sets with approximately valid coverage and small regret. However, standard regret minimization could be insufficient for handling changing environments, where performance guarantees may be desired not only over the full time horizon but also in all (sub-)intervals of time. We develop new online conformal prediction methods that minimize the strongly adaptive regret, which measures the worst-case regret over all intervals of a fixed length. We prove that our methods achieve near-optimal strongly adaptive regret for all interval lengths simultaneously, and approximately valid coverage. Experiments show that our methods consistently obtain better coverage and smaller prediction sets than existing methods on real-world tasks, such as time series forecasting and image classification under distribution shift.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {100},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}


@article{orabona2018scalefree,
  title = {Scale-free online learning},
  journal = {Theoretical Computer Science},
  volume = {716},
  pages = {50-69},
  year = {2018},
  note = {Special Issue on ALT 2015},
  issn = {0304-3975},
  doi = {https://doi.org/10.1016/j.tcs.2017.11.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397517308514},
  author = {Francesco Orabona and Dávid Pál},
  keywords = {Online algorithms, Optimization, Regret bounds, Online learning},
  abstract = {We design and analyze algorithms for online linear optimization that have optimal regret and at the same time do not need to know any upper or lower bounds on the norm of the loss vectors. Our algorithms are instances of the Follow the Regularized Leader (FTRL) and Mirror Descent (MD) meta-algorithms. We achieve adaptiveness to the norms of the loss vectors by scale invariance, i.e., our algorithms make exactly the same decisions if the sequence of loss vectors is multiplied by any positive constant. The algorithm based on FTRL works for any decision set, bounded or unbounded. For unbounded decisions sets, this is the first adaptive algorithm for online linear optimization with a non-vacuous regret bound. In contrast, we show lower bounds on scale-free algorithms based on MD on unbounded domains.}
}
